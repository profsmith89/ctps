

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Chapter 17: Build Prediction Models &#8212; Computational Thinking and Problem Solving</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chap17';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Chapter 18: Use Generative AI" href="chap18.html" />
    <link rel="prev" title="Chapter 16: Catch Them Early" href="chap16.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="ctps.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/mike_flat.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/mike_flat.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="ctps.html">
                    <no title>
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="welcome.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap01.html">Chapter 1: Read a Children’s Book</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap02.html">Chapter 2: Grab the Dialogue</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap03.html">Chapter 3: Replace Text With Emoji</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap04.html">Chapter 4: Query a Web Resource</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap05.html">Chapter 5: Play Guess-a-number</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap06.html">Chapter 6: Do You See My Dog?</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap07.html">Chapter 7: Many But Not Any Number</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap08.html">Chapter 8: What Is My Problem?</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap09.html">Chapter 9: Find a Phrase</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap10.html">Chapter 10: Build an Index</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap11.html">Chapter 11: Discover Driving Directions</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap12.html">Chapter 12: Divide and Conquer</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap13.html">Chapter 13: Rewrite the Error Message</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap14.html">Chapter 14: The Dream of Bug Fixing</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap15.html">Chapter 15: Embrace Runtime Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap16.html">Chapter 16: Catch Them Early</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Chapter 17: Build Prediction Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap18.html">Chapter 18: Use Generative AI</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="ales.html">Active-Learning Exercises</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="ales-c01.html">Chapter 1</a></li>





<li class="toctree-l2"><a class="reference internal" href="ales-c02.html">Chapter 2</a></li>







<li class="toctree-l2"><a class="reference internal" href="ales-c03.html">Chapter 3</a></li>









<li class="toctree-l2"><a class="reference internal" href="ales-c04.html">Chapter 4</a></li>





<li class="toctree-l2"><a class="reference internal" href="ales-c05.html">Chapter 5</a></li>
<li class="toctree-l2"><a class="reference internal" href="ales-c06.html">Chapter 6</a></li>
<li class="toctree-l2"><a class="reference internal" href="ales-c07.html">Chapter 7</a></li>
<li class="toctree-l2"><a class="reference internal" href="ales-c08.html">Chapter 8</a></li>
<li class="toctree-l2"><a class="reference internal" href="ales-c09.html">Chapter 9</a></li>



<li class="toctree-l2"><a class="reference internal" href="ales-c10.html">Chapter 10</a></li>


<li class="toctree-l2"><a class="reference internal" href="ales-c12.html">Chapter 12</a></li>



</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/chap17.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 17: Build Prediction Models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-home-prices">Predicting home prices</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#your-sister-s-data">Your sister’s data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-this-problem-ourselves">Solving this problem ourselves</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning">Machine learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#labeled-training-data">Labeled training data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ml-workflow">ML workflow</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-a-feel-for-the-data">Getting a feel for the data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#set-the-prediction-target">Set the prediction target</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pick-some-features">Pick some features</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fit-the-model-to-our-data">Fit the model to our data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-unseen-data">Predicting unseen data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-validation">Model validation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#making-the-fit-just-right">Making the fit just right</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-in-ml">Bias in ML</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classifying-comments-as-toxic">Classifying comments as toxic</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-art-than-science">More art than science</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="chapter-17-build-prediction-models">
<h1>Chapter 17: Build Prediction Models<a class="headerlink" href="#chapter-17-build-prediction-models" title="Permalink to this heading">#</a></h1>
<p>Computational tools help us to automate away the drudgery involved in our work, and they do this best when this work contains automatable patterns. If we can recognize them, we can build a script to repeatedly perform them. This is what we did in Chapter 2, when we hand-built a script to grab the dialogue in a story, and Chapter 13, when we used the regular-expression library to describe the more complicated patterns we saw in the Python interpreter’s error messages and rewrite them into a form easier to digest.</p>
<p><em>Pattern matching</em> is a broadly applicable technique, and in this chapter, I’ll introduce you to one of its most powerful uses: the building of predictive models. This returns us to the data science process introduced in Chapter 8, where we covered its first steps (i.e., data collection, data cleaning, and data exploration). I promised you that we’d return to cover its last steps (i.e., pattern detection and model building), and that time is now.</p>
<p>However, unlike earlier chapters, this time we won’t have to identify the patterns we wish to exploit. Because data scientists often work with data sets that are too large for humans to analyze, they need tools that can help to identify and exploit the patterns in these data sets. The design of these tools exist in a branch of artificial intelligence called <em>machine learning (ML)</em>, and we’ll look at one popular library for it called scikit-learn. With such a library, it is very easy to mine a data set for patterns and create a model that uses the patterns it learned to make predictions about new data. In effect, we’re asking the computer to analyze a data set and build an algorithm that we would find too difficult to build for ourselves.</p>
<p>We’ll explore two real-world problems in this space: (1) the predicting of housing prices based on a home’s characteristics (e.g., its number of bedrooms and bathrooms); and (2) the labeling of online comments as toxic. The first is classic and fairly straightforward ML application. We’ll use it as an introduction to ML tools and techniques. The second is a more difficult problem, and it will open our eyes to the dangers of bias in the ML models we build. I’ll mention two of the many ways in which bias can creep into a model so that you can begin checking your own. Unfortunately, eliminating bias remains a hard problem, and sometimes, when the harms a model creates are greater than its benefits, you simply shouldn’t deploy it.</p>
<div class="admonition-learning-outcomes admonition">
<p class="admonition-title">Learning Outcomes</p>
<p>Learn to use machine learning (ML) as a tool for discovering and exploiting real-world correlations. You will perform supervised learning to build a decision-tree model that that predicts home prices. You’ll practice with Pandas and data frames, i.e., table data structures. In addition to a focus on predictive accuracy, you’ll grapple with questions of bias in ML. After completing this chapter, you will be able to:</p>
<ul class="simple">
<li><p>Explain how ML works and when large data sets are required to build a good predictive model [design and CS concepts];</p></li>
<li><p>Use <a class="reference external" href="http://kaggle.com">kaggle.com</a> to find real-world data sets and example ML models [programming skills];</p></li>
<li><p>Describe the difference between supervised and unsupervised learning [CS concepts];</p></li>
<li><p>Work with the popular pandas and scikit-learn libraries to explore large data sets and create ML models [programming skills];</p></li>
<li><p>Validate the predictive accuracy of a ML model and quantity it using the Mean Absolute Error (MAE) metric [CS concepts and programming skills];</p></li>
<li><p>Decide when a ML model is appropriate to use and discuss some of the types of bias it might contain [design and CS concepts].</p></li>
</ul>
</div>
<section id="predicting-home-prices">
<p><strong>Predicting home prices.</strong> Imagine that your sister is a realtor in Iowa, and she’s found that she’s drowning each month in data about the housing market. This sounds like a problem that could be solved with computational thinking and a bit of programming, and so you say, “Don’t worry, sis! I’ve practiced computational thinking and problem solving. Send me the data, and I’ll build you a computational model based on past home sales. When a new seller calls you, just ask them to tell you about a few features of their home, feed that information into the model, and it will instantly spit out a market-appropriate price for their home.” She breaks into a huge smile and says that you’re now her favorite sibling.</p>
<p>This is our first problem to be solved, and it has three pieces:</p>
<ol class="arabic simple">
<li><p>building a computational model that considers a home’s features and uses them to predict a selling price;</p></li>
<li><p>tuning the model to produce good predictions; and</p></li>
<li><p>using the tuned model to predict the prices of previously unseen homes in a business setting.</p></li>
</ol>
<p>We’ll focus on the first two pieces, and your sister will use our tuned model in her business.</p>
</section>
<section id="your-sister-s-data">
<p><strong>Your sister’s data.</strong> Let’s assume that your sister lives in Ames, Iowa. In 2011, Dean De Cock published a data set describing the housing sales in this area from 2006 to 2010.<a class="footnote-reference brackets" href="#fn1" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> He published it for use in undergraduate regression courses, and it has since become a popular data set for learning about ML. We’ll grab a copy of this data set from the online platform <a class="reference external" href="https://www.kaggle.com">kaggle.com</a>, where you can join a community of ML enthusiasts and peruse a repository of community-published data sets and ML models.</p>
<p>The Ames-Iowa-Housing data is one large CSV file. Recall that a CSV is a text file that contains row after row of comma-separated values. It is tabular data that you could read into a spreadsheet application, but let’s practice our Python and write a small script (<code class="docutils literal notranslate"><span class="pre">head.py</span></code>) to look at the first five rows of this data set.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="c1">### chap17/head.py</span>
<span class="linenos"> 2</span><span class="kn">import</span> <span class="nn">sys</span><span class="o">,</span> <span class="nn">csv</span>
<span class="linenos"> 3</span>
<span class="linenos"> 4</span><span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
<span class="linenos"> 5</span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
<span class="linenos"> 6</span>        <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="s1">&#39;Usage: head.py input.csv&#39;</span><span class="p">)</span>
<span class="linenos"> 7</span>
<span class="linenos"> 8</span>    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fin</span><span class="p">:</span>
<span class="linenos"> 9</span>        <span class="n">reader</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">DictReader</span><span class="p">(</span><span class="n">fin</span><span class="p">)</span>
<span class="linenos">10</span>        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">reader</span><span class="p">):</span>
<span class="linenos">11</span>            <span class="c1"># Just print the first 5 rows</span>
<span class="linenos">12</span>            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="p">:</span>
<span class="linenos">13</span>                <span class="nb">print</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>
<span class="linenos">14</span>            <span class="k">else</span><span class="p">:</span>
<span class="linenos">15</span>                <span class="k">break</span>
<span class="linenos">16</span>
<span class="linenos">17</span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
<span class="linenos">18</span>    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p>This script uses the <code class="docutils literal notranslate"><span class="pre">csv</span></code> library and its <code class="docutils literal notranslate"><span class="pre">DictReader</span></code> function, which takes a table’s column headings and uses them as dictionary keys. When you run <code class="docutils literal notranslate"><span class="pre">head.py</span></code> on your copy of the Ames-Iowa-Housing CSV file, you should see the first five data rows printed as dictionaries. Each row is a home sale, which lists a lot of information. My copy of the data set contained 82 different pieces of information about each home sale, including <code class="docutils literal notranslate"><span class="pre">'Year</span> <span class="pre">Built'</span></code> and <code class="docutils literal notranslate"><span class="pre">'SalePrice'</span></code>. We now know why your sister asked for help.</p>
<div class="admonition-you-try-it admonition">
<p class="admonition-title">You Try It</p>
<p>Download the Ames-Iowa-Housing data from <a class="reference external" href="http://kaggle.com">kaggle.com</a> and run <code class="docutils literal notranslate"><span class="pre">head.py</span></code> on the complete CSV file.</p>
</div>
</section>
<section id="solving-this-problem-ourselves">
<p><strong>Solving this problem ourselves.</strong> We don’t know much about selling houses, but your sister has said that homes with more bedrooms sell for more money. Ok, we know about making decisions like this: we simply need to use some if-statements. Maybe we can write something like the following, where I’ve indicated the relationship between the predicted prices but didn’t specify their actual values.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="c1">### Pseudocode implementing a simple decision tree</span>
<span class="linenos">2</span>
<span class="linenos">3</span><span class="c1"># Grab data about new seller&#39;s home</span>
<span class="linenos">4</span>
<span class="linenos">5</span><span class="c1"># Predict price based on number of bedrooms</span>
<span class="linenos">6</span><span class="c1"># if home.bedrooms &gt; 2:</span>
<span class="linenos">7</span><span class="c1">#     print(&#39;Predicted price:&#39;, A_BIG_NUMBER)</span>
<span class="linenos">8</span><span class="c1"># else:</span>
<span class="linenos">9</span><span class="c1">#     print(&#39;Predicted price:&#39;, A_SMALL_NUMBER)</span>
</pre></div>
</div>
<p>To figure out what the predicted prices should be, let’s write a Python script that analyzes the Ames-Iowa-Housing data. This script, which I’ve called <code class="docutils literal notranslate"><span class="pre">avg_pbbr.py</span></code> (i.e., average price by bedroom), buckets the previously sold homes by their number of bedrooms and then calculates and prints each bucket’s average sale price. Because it uses a fixed size array, it is careful to verify that there are no homes in the data set with more than nine bedrooms. It also demonstrates yet another way to do some formatted printing.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="c1">### chap17/avg_pbbr.py</span>
<span class="linenos"> 2</span><span class="kn">import</span> <span class="nn">sys</span><span class="o">,</span> <span class="nn">csv</span>
<span class="linenos"> 3</span>
<span class="linenos"> 4</span><span class="c1"># Each `homes` element is a tuple with these elements:</span>
<span class="linenos"> 5</span><span class="c1"># 1. count of homes with a number of bedrooms equal to the index</span>
<span class="linenos"> 6</span><span class="c1"># 2. total sales price for all such homes</span>
<span class="linenos"> 7</span><span class="n">MAX_BEDROOMS</span> <span class="o">=</span> <span class="mi">9</span>
<span class="linenos"> 8</span><span class="n">homes</span> <span class="o">=</span> <span class="p">[</span>
<span class="linenos"> 9</span>    <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
<span class="linenos">10</span>    <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
<span class="linenos">11</span>    <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
<span class="linenos">12</span><span class="p">]</span>
<span class="linenos">13</span>
<span class="linenos">14</span><span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
<span class="linenos">15</span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
<span class="linenos">16</span>        <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="s1">&#39;Usage: avg_pbbr.py input.csv&#39;</span><span class="p">)</span>
<span class="linenos">17</span>
<span class="linenos">18</span>    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fin</span><span class="p">:</span>
<span class="linenos">19</span>        <span class="n">reader</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">DictReader</span><span class="p">(</span><span class="n">fin</span><span class="p">)</span>
<span class="linenos">20</span>        <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">reader</span><span class="p">:</span>
<span class="linenos">21</span>            <span class="c1"># Grab this entry&#39;s number of bedrooms</span>
<span class="linenos">22</span>            <span class="n">num_bedrooms</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;Bedroom AbvGr&#39;</span><span class="p">])</span>
<span class="linenos">23</span>            <span class="k">assert</span> <span class="n">num_bedrooms</span> <span class="o">&lt;=</span> <span class="n">MAX_BEDROOMS</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;We need a bigger list (</span><span class="si">{</span><span class="n">num_bedrooms</span><span class="si">}</span><span class="s2">)&quot;</span>
<span class="linenos">24</span>
<span class="linenos">25</span>            <span class="c1"># Grab its sales price</span>
<span class="linenos">26</span>            <span class="n">price</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;SalePrice&#39;</span><span class="p">])</span>
<span class="linenos">27</span>
<span class="linenos">28</span>            <span class="c1"># Update the right tuple in `bedrooms`</span>
<span class="linenos">29</span>            <span class="n">homes</span><span class="p">[</span><span class="n">num_bedrooms</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
<span class="linenos">30</span>                <span class="n">homes</span><span class="p">[</span><span class="n">num_bedrooms</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
<span class="linenos">31</span>                <span class="n">homes</span><span class="p">[</span><span class="n">num_bedrooms</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">price</span>
<span class="linenos">32</span>            <span class="p">)</span>
<span class="linenos">33</span>
<span class="linenos">34</span>        <span class="c1"># Print out the results</span>
<span class="linenos">35</span>        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">homes</span><span class="p">):</span>
<span class="linenos">36</span>            <span class="k">if</span> <span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
<span class="linenos">37</span>                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Avg. sale price of </span><span class="si">{</span><span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s1">&gt;</span><span class="si">{</span><span class="mi">4</span><span class="si">}}</span><span class="s1"> homes with </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1"> bedrooms: &#39;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;$&#39;</span><span class="p">)</span>
<span class="linenos">38</span>                <span class="nb">print</span><span class="p">(</span><span class="nb">format</span><span class="p">(</span><span class="n">t</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;.2f&#39;</span><span class="p">))</span>
<span class="linenos">39</span>
<span class="linenos">40</span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
<span class="linenos">41</span>    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition-you-try-it admonition">
<p class="admonition-title">You Try It</p>
<p>Run <code class="docutils literal notranslate"><span class="pre">avg_pbbr.py</span></code> on the CSV file you previously downloaded, and you’ll see that there’s no observable pattern in the printed averages.</p>
</div>
<p>Perhaps a pattern will emerge if we include additional home features, like lot size. We’d extend our earlier pseudocode as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="c1">### Pseudocode with a larger decision tree</span>
<span class="linenos"> 2</span>
<span class="linenos"> 3</span><span class="c1"># Grab data about new seller&#39;s home</span>
<span class="linenos"> 4</span>
<span class="linenos"> 5</span><span class="c1"># Predict price based on number of bedrooms and lot size</span>
<span class="linenos"> 6</span><span class="c1"># if home.bedrooms &gt; 2:</span>
<span class="linenos"> 7</span><span class="c1">#     if home.lotarea &gt; 12000: </span>
<span class="linenos"> 8</span><span class="c1">#         print(&#39;Predicted price:&#39;, A_BIG_NUMBER)</span>
<span class="linenos"> 9</span><span class="c1">#     else:</span>
<span class="linenos">10</span><span class="c1">#         print(&#39;Predicted price:&#39;, A_NOT_AS_BIG_NUMBER)</span>
<span class="linenos">11</span><span class="c1"># else:</span>
<span class="linenos">12</span><span class="c1">#     if home.lotarea &gt; 12000: </span>
<span class="linenos">13</span><span class="c1">#         print(&#39;Predicted price:&#39;, A_SMALL_NUMBER)</span>
<span class="linenos">14</span><span class="c1">#     else:</span>
<span class="linenos">15</span><span class="c1">#         print(&#39;Predicted price:&#39;, A_SMALLER_NUMBER)</span>
</pre></div>
</div>
<p>What we’re building is called a <em>decision tree</em>. For each characteristic we think is important in determining the predicted price, we include a node in the tree (i.e., an if-statement) that directs us toward a subtree (i.e., subsequent if-statements) where that characteristic is true. When we get to the leaves of the tree (i.e., when we run out of conditions to check), we print the predicted price for a house with those characteristics. In our pseudocode, for instance, <code class="docutils literal notranslate"><span class="pre">A_SMALL_NUMBER</span></code> is the predicted price for a home with two or fewer bedrooms on a lot of more than 12,000 square feet.</p>
<p>Unfortunately, even if we were patient enough to figure out what buckets of lot sizes to use, we’d still find that this decision tree doesn’t use enough of the data to create a good predictor. We need to a smarter approach. We need <em>machine learning</em>.</p>
</section>
<section id="machine-learning">
<p><strong>Machine learning.</strong> We’ve been trying to suss out a pattern in the Ames-Iowa-Housing data that we could use as the foundation for a good predictor, and we were using your sister’s experience in real estate as a starting point in this search. This wasn’t a bad idea since evolution has made humans into good pattern recognizers—as we discussed in Chapter 1, the accumulation of experience turns good programmers into great ones (i.e., individuals that seem to quickly jump to the “right” structure for a script to solve a new problem). But we don’t have your sister’s experience and the patterns in the Ames-Iowa-Housing data might be quite subtle. ML is a computational approach that enables computers, through the use of <em>statistical techniques</em> and <em>a large training data set</em>, to build <em>models</em> that perform like an experienced human. They allow all of us to become instant experts in a new domain.</p>
<p>The statistical technique we used a moment ago was a decision tree. Statisticians recommend this approach when the relationship between the <em>features</em> (i.e., the characteristics of a home such as its number of bedrooms and lot size) and the <em>target variable</em> (i.e., the home’s selling price) is complex. For those of you that know a bit about statistical analysis, this often means that the relationship is non-linear or includes important outliers.</p>
<p>Decision trees produce fine predictors of home prices, and yet there are other statistical techniques that yield more accurate predictions. Despite this, we’ll continue using decision trees. They are a great starting place for learning how to do ML because they are: (1) easy to understand; and (2) they form the basic building block of several better techniques.</p>
</section>
<section id="labeled-training-data">
<p><strong>Labeled training data.</strong> I said that ML relies on statistical techniques and a <em>large training</em> data set. We have been using the prices for past home sales in Ames, Iowa as accurate indicators of the prices of future home sales in that area. In other words, these past home sales are the experience we need to predict future home sales, and we can therefore use these past home sales to <em>train</em> our ML model. You’ll often see this described as <em>fitting</em> a model to the training data.</p>
<p>When there is a strong correlation between a feature (e.g., number of bedrooms) and the target variable (e.g., selling price), we might need to see very few training examples to build a good prediction model. However, we learned using <code class="docutils literal notranslate"><span class="pre">avg_pbbr.py</span></code> that there isn’t a strong correlation between the number of bedrooms and selling price in the Ames-Iowa-Housing data. But since the real estate industry uses comparable houses to set prices, there must be some correlation between the features of a home and its sale price. We simply need a large enough data set to discover that correlation.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>When the patterns are hard to see, you’ll need a large training set to have ML create a good predictor.</p>
</div>
<p>There’s one other characteristic of the Ames-Iowa-Housing data that doesn’t have to be true in every ML problem: these data are <em>labeled</em>. This term means that the data set includes the answers, i.e., the value of the <em>target variable</em>, which for our problem is the selling price. In other words, we know not only the values of the features of each home, but we also know its true selling price. When using labeled data, it is called <em>supervised learning</em>.</p>
<p>But not all ML requires labeled data. When we want to analyze a data set for any kind of pattern, it is called <em>unsupervised learning</em>. We won’t talk further about this type of ML except to say that you should investigate it if you’re interested in problems such as natural language processing, object recognition, customer segmentation (e.g., recommendation engines), or exploratory data analysis.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The discipline of ML includes a dizzying number of statistical techniques under the headings of supervised and unsupervised learning. As you work in this area, you’ll learn that there is no single best method, and you’ll find that no one can accurately predict whether a particular approach will be successful. Experience will help limit the amount of trial and error you’ll do, but be prepared to try lots of different approaches until you find one that works well.</p>
</div>
</section>
<section id="ml-workflow">
<p><strong>ML workflow.</strong> We’re about ready to dig into a ML library and solve our sister’s problem, but before we discuss its particulars, I want to describe in general how we’re moving work off our plates and on to the machine’s. At the start of the chapter, we tried to follow a process like this:</p>
<ol class="arabic simple">
<li><p>We looked at the data and picked one or more features we thought might correlate well with the target variable.</p></li>
<li><p>We wrote a script (e.g., <code class="docutils literal notranslate"><span class="pre">avg_pbbr.py</span></code>) that analyzed the data in the data set to see if a pattern existed between our chosen features and the target variable. If not, we returned to step 1.</p></li>
<li><p>Using what we learned from the analysis script, we would have filled in the unknown numbers in our decision-tree pseudocode and written a script that takes as input a set of feature values and outputs a predicted target value. This would have been the “model” we would have sent to your sister.</p></li>
</ol>
<p>In using this model, your sister probably would find that it didn’t do an outstanding job of predicting the selling price of a new home coming on the market. This is because the steps we took didn’t involve looking for the <em>best</em> model we could have built; we simply built <em>a</em> model based on the first pattern we found. An experienced data scientist would iterate and ask, “Ok, my first model works, but is there a better selection of features (or even a superior statistical approach) that produces a better predictor?”</p>
<p>A good ML library automates much of steps 2 and 3 for you. We’ll use the <code class="docutils literal notranslate"><span class="pre">pandas</span></code> and scikit-learn libraries, and with their help, this process turns into the following:</p>
<ol class="arabic simple">
<li><p>Review and analyze our data set—we’ll use the <code class="docutils literal notranslate"><span class="pre">pandas</span></code> library for this work—and pick one or more features we think might correlate well with the target variable.<a class="footnote-reference brackets" href="#fn2" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a></p></li>
<li><p>Select a type of statistical analysis that’ll lie at the heart of our model. We’ll use the <code class="docutils literal notranslate"><span class="pre">DecisionTreeRegressor</span></code> class in scikit-learn. Besides decision trees, this library supports many other statistical techniques, and we’ll employ a different one when we later consider a predictor for the toxicity of online comments.</p></li>
<li><p>Create an instance of the <code class="docutils literal notranslate"><span class="pre">DecisionTreeRegressor</span></code> class and call the resulting object’s <code class="docutils literal notranslate"><span class="pre">fit</span></code> method, giving it the set of features and the target variable we want it to use from our training data.</p></li>
<li><p>Test this object, which is a model fit to our training data via supervised learning, against some previously unseen data, and evaluate whether this model is a good one. If it is not, we’ll return to an earlier step, changing perhaps the features included or the type of statistical analysis performed.</p></li>
<li><p>Share our best model with your sister.</p></li>
</ol>
<p>Notice that we no longer need to write the code that implements a statistical analysis (step 2) or the model that fits the training data (step 3), as we tried to do earlier. Using powerful libraries like <code class="docutils literal notranslate"><span class="pre">pandas</span></code> and scikit-learn, our scripts need make only a few function calls.</p>
</section>
<section id="getting-a-feel-for-the-data">
<p><strong>Getting a feel for the data.</strong> With this foundation, you are ready to start using the tools of a data scientist. We’ll begin with the pandas library, which “is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.”<a class="footnote-reference brackets" href="#fn3" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a> It provides data structures and functions that allow us to quickly explore a data set.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Many data scientists work in interactive Python notebooks (i.e., <code class="docutils literal notranslate"><span class="pre">ipynb</span></code> files) when doing ML, and we’ll do the same in the rest of this chapter. They also commonly refer to the <code class="docutils literal notranslate"><span class="pre">pandas</span></code> library with the abbreviation <code class="docutils literal notranslate"><span class="pre">pd</span></code>.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="c1">### chap17/ames.ipynb, 4th code block</span>
<span class="linenos">2</span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>
</div>
<p>The most important data structure in the <code class="docutils literal notranslate"><span class="pre">pandas</span></code> library is the <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code>. You can think of a <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code> as resembling a table, like a worksheet in Excel notebook. The <code class="docutils literal notranslate"><span class="pre">pandas</span></code> library provides input/output functions, like <code class="docutils literal notranslate"><span class="pre">pandas.read_csv</span></code>, that allow you to pour data into and pull data out of a <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code>. And once your data is in a <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code>, you can apply many of the library’s powerful functions, such as <code class="docutils literal notranslate"><span class="pre">pandas.DataFrame.describe</span></code> that creates a summary description of the data in each column. The following code block shows how we can use the <code class="docutils literal notranslate"><span class="pre">pandas</span></code> library to begin exploring the data set containing home sales in Ames, Iowa.</p>
<div class="tip admonition">
<p class="admonition-title">Terminology</p>
<p>Each column in a <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code> is a <code class="docutils literal notranslate"><span class="pre">pandas.Series</span></code> data structure, and data scientists will often refer to a column as a series.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="c1">### chap17/ames.ipynb, 3rd and 5th code blocks</span>
<span class="linenos">2</span><span class="c1"># Specify the input CSV file for building and testing the model</span>
<span class="linenos">3</span><span class="n">csv_file</span> <span class="o">=</span> <span class="s1">&#39;AmesIowaHousingData/AmesHousing.csv&#39;</span>
<span class="linenos">4</span>
<span class="linenos">5</span><span class="c1"># Read the CSV data into a DataFrame</span>
<span class="linenos">6</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">csv_file</span><span class="p">)</span>
<span class="linenos">7</span>
<span class="linenos">8</span><span class="c1"># Print a summary of the data</span>
<span class="linenos">9</span><span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition-you-try-it admonition">
<p class="admonition-title">You Try It</p>
<p>Set the variable <code class="docutils literal notranslate"><span class="pre">csv_file</span></code> to the location of where you placed the Ames-Iowa-Housing data set. Execute the referenced code blocks in <code class="docutils literal notranslate"><span class="pre">ames.ipynb</span></code> as you read through this chapter. Remember that the interactive Python interpreter will print the result of the last statement in a code block when you execute it in a <code class="docutils literal notranslate"><span class="pre">ipynb</span></code> file. You’ll need to see the returned result for <code class="docutils literal notranslate"><span class="pre">df.describe()</span></code> to follow the explanation that comes next.</p>
</div>
<p>The results of our call to <code class="docutils literal notranslate"><span class="pre">df.describe()</span></code> list eight numbers for a large number of our data set’s columns. The <code class="docutils literal notranslate"><span class="pre">pandas</span></code> library decided that each of these columns is a <em>numeric</em> series, and the numbers under the series name tell you some statistical facts about each of these columns. Here’s a short summary of the meaning of each row in the table returned by <code class="docutils literal notranslate"><span class="pre">describe</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">count</span></code> reports the number of rows (in the indicated column from the data set) with <em>non-missing values</em>. As we discussed in Chapter 8, there are many reasons why a data set may contain missing values. In our data set, for example, two of the home sales don’t report any value for <code class="docutils literal notranslate"><span class="pre">'Bsmt</span> <span class="pre">Full</span> <span class="pre">Bath'</span></code>, which might have occurred because the person filling out the data did’t think that they needed to fill out this column for a house without a basement.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mean</span></code> is the arithmetic average of the non-missing values.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">std</span></code> is the standard deviation of the non-missing values, and it provides a measure of the values’ numerical spread.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">min</span></code>, <code class="docutils literal notranslate"><span class="pre">25%</span></code>, <code class="docutils literal notranslate"><span class="pre">50%</span></code>, <code class="docutils literal notranslate"><span class="pre">75%</span></code> and <code class="docutils literal notranslate"><span class="pre">max</span></code> are best considered together. If you were to sort the non-missing values in the column from the smallest number to the largest, the first is <code class="docutils literal notranslate"><span class="pre">min</span></code> and the last <code class="docutils literal notranslate"><span class="pre">max</span></code>. Then imagine drawing three lines that split the sorted list into four equal-sized buckets. The number at these three split points are the values at the 25th percentile (25%), the 50th percentile (50%), and the 75th percentile (75%). Intuitively, the 25th percentile is the number that is bigger than a quarter of the column’s values and smaller than the other three-quarters.</p></li>
</ul>
<p>Knowing this, I can see that my copy of the Ames-Iowa-Housing data contains 2930 entries and the average home sale price between 2006 and 2010 was approximately 180,800 dollars, with the cheapest home selling for just under 80,000 dollars and the most expensive one for 755,000 dollars.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">pandas</span></code> library limits the number of <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code> columns and rows it displays, and if we wanted to see the statistics about bedrooms above ground (<code class="docutils literal notranslate"><span class="pre">'Bedroom</span> <span class="pre">AbvGr'</span></code>) that we analyzed with our own script earlier, we’d either have to change the maximum number of columns that <code class="docutils literal notranslate"><span class="pre">pandas</span></code> displays or slice that series from the result of the <code class="docutils literal notranslate"><span class="pre">describe</span></code> method call. Let’s choose the latter approach:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="c1">### chap17/ames.ipynb, 7th code block</span>
<span class="linenos">2</span><span class="c1"># Review only the statistics for number of bedrooms</span>
<span class="linenos">3</span><span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">()[</span><span class="s1">&#39;Bedroom AbvGr&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>From this result, we can see that 2- and 3-bedroom houses are common, and that we didn’t need to worry about tracking houses with more than eight bedrooms.</p>
</section>
<section id="set-the-prediction-target">
<p><strong>Set the prediction target.</strong> We’re now ready to define the setup we’ll use in configuring our prediction model. Let’s start with the model’s output, which is the variable we want to predict. The series labeled <code class="docutils literal notranslate"><span class="pre">'SalePrice'</span></code> is this target variable, and by convention, data scientists call it <code class="docutils literal notranslate"><span class="pre">y</span></code>.</p>
<p>To set what <code class="docutils literal notranslate"><span class="pre">y</span></code> names, we can grab that series from our <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code> using one of two notations: (1) square brackets to slice it out, as we’ve done with other sequences; or (2) what’s called <em>dot notation</em> in <code class="docutils literal notranslate"><span class="pre">pandas</span></code>. The former always works, while the latter is a nice shorthand when the series name doesn’t contain spaces or conflict with another attribute name in the <code class="docutils literal notranslate"><span class="pre">pandas</span></code> library. The following code block illustrates both methods while using the latter since this label mets the requirements for using the shorthand.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="c1">### chap17/ames.ipynb, 8th code block</span>
<span class="linenos">2</span><span class="c1"># Setting the prediction target</span>
<span class="linenos">3</span><span class="c1"># y = df[&#39;SalePrice&#39;]</span>
<span class="linenos">4</span><span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">SalePrice</span>
<span class="linenos">5</span><span class="n">y</span>
</pre></div>
</div>
</section>
<section id="pick-some-features">
<p><strong>Pick some features.</strong> As we did when we tried to solve this problem by writing our own code, we need to choose which features we think will correlate nicely with our target variable. Almost every series is a candidate except the first, which in my copy of the data set is called <code class="docutils literal notranslate"><span class="pre">'Order'</span></code>, and the last, which is the target variable. I eliminate the first series from contention since it is nothing more than each record’s index in the data set. You’d want to eliminate anything like it when it’s clear that that series could not possibly correlate with the target variable.</p>
<p>To choose among the others, let’s interrogate the <code class="docutils literal notranslate"><span class="pre">columns</span></code> attribute on a <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code>, which prints all the column labels.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="c1">### chap17/ames.ipynb, 9th code block</span>
<span class="linenos">2</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span>
</pre></div>
</div>
<p>As you’ll find in many big data sets, there are a lot of columns to consider. While it might be tempting to include all the eligible columns in your model, that’s not always the best choice. Let’s begin with just a few as we can always later change the list of features included and compare the performance of different models.</p>
<p>In the next code block, we build a list of column names that represent a collection of features we want included in our model, and then use this list to slice those series out of our <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code>. Again, by convention, these data are called <code class="docutils literal notranslate"><span class="pre">X</span></code>. This and the following code block invoke the <code class="docutils literal notranslate"><span class="pre">describe</span></code> and <code class="docutils literal notranslate"><span class="pre">head</span></code> methods on the <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code> called <code class="docutils literal notranslate"><span class="pre">X</span></code> to check what we’ve done.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="c1">### chap17/ames.ipynb, 10th code block</span>
<span class="linenos">2</span><span class="c1"># The model&#39;s input features</span>
<span class="linenos">3</span><span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Lot Area&quot;</span><span class="p">,</span> <span class="s2">&quot;Year Built&quot;</span><span class="p">,</span> <span class="s2">&quot;1st Flr SF&quot;</span><span class="p">,</span> <span class="s2">&quot;2nd Flr SF&quot;</span><span class="p">,</span> <span class="s2">&quot;Full Bath&quot;</span><span class="p">,</span> <span class="s2">&quot;Bedroom AbvGr&quot;</span><span class="p">,</span> <span class="s2">&quot;TotRms AbvGrd&quot;</span><span class="p">]</span>
<span class="linenos">4</span><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span>
<span class="linenos">5</span><span class="n">X</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="c1">### chap17/ames.ipynb, 11th code block</span>
<span class="linenos">2</span><span class="n">X</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>As we’ve learned, frequent checks of our script’s state help us to find mistakes in our logic. These sorts of checks are even more important in the work of a data scientist, since it is harder to spot a model built from non-sensical data. Check the contents of your data frames frequently!</p>
</div>
</section>
<section id="fit-the-model-to-our-data">
<p><strong>Fit the model to our data.</strong> We chose to use a decision tree, which the scikit-learn library<a class="footnote-reference brackets" href="#fn4" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a> provides through the <code class="docutils literal notranslate"><span class="pre">DecisionTreeRegressor</span></code> class. Instantiating an instance of this class is the first step in creating a model that uses decision trees.</p>
<p>Hidden in this creation step, however, is a bit of randomness, which many ML libraries use as a best practice. Since we want to compare the models we build against each other, we want to eliminate this randomness as a potential cause of any difference in the performance of our models. We can accomplish this by specifying the random seed that the model should use each time it creates a new instance for us. This is the purpose of the <code class="docutils literal notranslate"><span class="pre">random_state</span></code> assignment in the constructor call in the following code block. You can choose any number you’d like as long as you consistently use it in all your constructor calls.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="c1">### chap17/ames.ipynb, 12th code block</span>
<span class="linenos">2</span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="linenos">3</span>
<span class="linenos">4</span><span class="c1"># Create an untrained model</span>
<span class="linenos">5</span><span class="n">my_model</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="linenos">6</span>
<span class="linenos">7</span><span class="c1"># Fit the model to the specified portion of the training data</span>
<span class="linenos">8</span><span class="n">my_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>Given a newly created instance of <code class="docutils literal notranslate"><span class="pre">DecisionTreeRegressor</span></code>, we simply pass our <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> variables to this object’s <code class="docutils literal notranslate"><span class="pre">fit</span></code> method, and it updates the model. That was easy! We’ll now use this object to make predictions.</p>
</section>
<section id="predicting-unseen-data">
<p><strong>Predicting unseen data.</strong> To make a prediction using this fitted model, we need a set of values for the model’s input features. Inputting these feature values will cause the model to return a prediction for the target variable. In other words, have someone tell us about a house we haven’t yet seen, and for that house, give us its lot size, year it was built, square feet of living space in the first and second floors, and the number of full bathrooms, bedrooms above ground, and total rooms above ground. That’s the list of features we used to train our model to predict selling prices.</p>
<p>Ok, but we used all the data we had about home sales in Ames, Iowa to create the model. We could go back to your sister for more data, but there’s another way: data scientists take a given data set and split it in two. One part of this split is designated the <em>training set</em> and the other is the <em>test (or testing) set</em>. We’ll use the former to create our model and the latter to check its performance.</p>
<p>There are many ways to poorly split a data set, and to avoid these pitfalls, the train-test split routines in ML libraries like scikit-learn employ randomness. Again, we want to control the random seed used by these functions so that we can ensure that all the models we build are fed the same training and testing data sets.</p>
<p>The code block below retrains our model using the <code class="docutils literal notranslate"><span class="pre">train_X</span></code> and <code class="docutils literal notranslate"><span class="pre">train_y</span></code> data sets produced by the <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> function in the scikit-learn library. It then illustrates how to use the <code class="docutils literal notranslate"><span class="pre">predict</span></code> method on our model object, into which we feed the <code class="docutils literal notranslate"><span class="pre">test_X</span></code> data set. The resulting <code class="docutils literal notranslate"><span class="pre">predictions</span></code> are what we want to compare against the actual sale prices stored in <code class="docutils literal notranslate"><span class="pre">test_y</span></code>. I’ve written a little loop that does this comparison for the first five predictions, converting a few data types along the way (i.e.., the <code class="docutils literal notranslate"><span class="pre">Series</span></code> in <code class="docutils literal notranslate"><span class="pre">test_y</span></code> into a Python list and each <code class="docutils literal notranslate"><span class="pre">float</span></code> in <code class="docutils literal notranslate"><span class="pre">predictions</span></code> into an <code class="docutils literal notranslate"><span class="pre">int</span></code>).</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Don’t test with your training data. A model’s utility is defined by its ability to make accurate predictions on data it hasn’t previously seen, which are called the <em>validation data</em>. It’s not hard to get good predictions from a model’s training data.<a class="footnote-reference brackets" href="#fn5" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="c1">### chap17/ames.ipynb, 13th code block</span>
<span class="linenos"> 2</span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="linenos"> 3</span>
<span class="linenos"> 4</span><span class="c1"># Split both features and target data into training and validation sets</span>
<span class="linenos"> 5</span><span class="n">train_X</span><span class="p">,</span> <span class="n">test_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">test_y</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="linenos"> 6</span>
<span class="linenos"> 7</span><span class="c1"># Fit the model using the training data</span>
<span class="linenos"> 8</span><span class="n">my_model</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="linenos"> 9</span><span class="n">my_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>
<span class="linenos">10</span>
<span class="linenos">11</span><span class="c1"># Feed the model the test data and capture the resulting predictions</span>
<span class="linenos">12</span><span class="n">predictions</span> <span class="o">=</span> <span class="n">my_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_X</span><span class="p">)</span>
<span class="linenos">13</span>
<span class="linenos">14</span><span class="c1"># Compare first 5 predictions against actual sale prices</span>
<span class="linenos">15</span><span class="n">actuals</span> <span class="o">=</span> <span class="n">test_y</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>
<span class="linenos">16</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
<span class="linenos">17</span>    <span class="n">p</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="linenos">18</span>    <span class="n">a</span> <span class="o">=</span> <span class="n">actuals</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="linenos">19</span>    <span class="n">d</span> <span class="o">=</span> <span class="n">p</span> <span class="o">-</span> <span class="n">a</span>
<span class="linenos">20</span>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;prediction = $</span><span class="si">{</span><span class="n">p</span><span class="si">}</span><span class="s1">; actual = $</span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s1">; diff = $</span><span class="si">{</span><span class="n">d</span><span class="si">:</span><span class="s1">&gt;6</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition-you-try-it admonition">
<p class="admonition-title">You Try It</p>
<p>Run the code above. Are you happy with the resulting predictions? Did we build a good model for your sister?</p>
</div>
</section>
<section id="model-validation">
<p><strong>Model validation.</strong> <em>Predictive accuracy</em> is the first measure that data scientists consider when deciding whether a model is good. If a model correctly predicts the target variable a sizable percentage of the time, they say it has good predictive accuracy. If it doesn’t, they throw the model away and try again.</p>
<p>You’re probably thinking: How often does the model need to be correct, i.e., what percentage classifies as “sizable”? The answer depends on how you’ll use the model. For your sister’s use case (i.e., predicting home prices), her reputation will be damaged if the sale prices she suggests to her clients are far from what other experienced realtors suggest. But being slightly off the actual sales price isn’t a big deal; no one expects a home to sell for exactly its listing price. This means that we want a reasonably high predictive accuracy, e.g., 70-90 percent of the time the model predicts the selling price within a couple of thousand dollars.<a class="footnote-reference brackets" href="#fn6" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a> On the other hand, you probably wouldn’t be happy with a predictive accuracy of 9 in 10 if your doctor was using the model to decide whether your X-ray showed a tumor.</p>
<p>Ok, but how do we check if the model we built meets this 7-to-9-in-ten target? When I ran the previous code block, which compared the first five predicted home prices against the actual prices these homes sold for, it printed the following:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span>prediction = $231000; actual = $161000; diff = $ 70000
<span class="linenos">2</span>prediction = $100500; actual = $116000; diff = $-15500
<span class="linenos">3</span>prediction = $193000; actual = $196500; diff = $ -3500
<span class="linenos">4</span>prediction = $138000; actual = $123600; diff = $ 14400
<span class="linenos">5</span>prediction = $103000; actual = $126000; diff = $-23000
</pre></div>
</div>
<p>Not one of the predictions matched the actual selling price; the model made errors on both sides of the actual price; and the first prediction is particularly bad. As humans, we have a hard time digesting just these five results, and it will be impossible for us to mentally process all the differences in the full testing set. We need a single metric (or no more than a few metrics) that the computer can compute for us that will indicate how well our model did overall.</p>
<p>In practice, there are many such metrics, and we’ll use one of the most popular, which is called the <em>Mean Absolute Error (MAE)</em>. Intuitively, MAE indicates how far off, on average, the model’s predictions are from the actual values. While we might be interested in knowing if the model is always wrong in a particular direction (e.g., the predicted value is always larger than the actual), this metric throws away that information by first taking the absolute value of the difference between each predicted and actual value. Then from these magnitudes, it computes a simple arithmetic average and reports that as the model’s MAE on the test set. The scikit-learn library provides a function that computes MAE:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="c1">### chap17/ames.ipynb, 14th code block</span>
<span class="linenos"> 2</span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span>
<span class="linenos"> 3</span>
<span class="linenos"> 4</span><span class="n">test_mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">test_y</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
<span class="linenos"> 5</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;MAE = $</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">test_mae</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="linenos"> 6</span>
<span class="linenos"> 7</span><span class="c1"># Compare the MAE against the average home price</span>
<span class="linenos"> 8</span><span class="n">test_mean</span> <span class="o">=</span> <span class="n">test_y</span><span class="o">.</span><span class="n">describe</span><span class="p">()[</span><span class="s1">&#39;mean&#39;</span><span class="p">]</span>
<span class="linenos"> 9</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Mean price = $</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">test_mean</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="linenos">10</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Percentage of price = </span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">test_mae</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">test_mean</span><span class="p">)</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Our first attempt at building a model for your sister produced one with a MAE that’s about 16 percent of the test set’s average home price. Not great, but not too bad for our first try!</p>
</section>
<section id="making-the-fit-just-right">
<p><strong>Making the fit just right.</strong> As we discussed earlier, we don’t want to send the first model we get working to your sister. We want to vary how we build the model and send her the best one. If you continue in data science, you’ll quickly learn that there are many choices you make in building a model and changing these choices affects the model’s quality. Some of these choices are absolutes, like “I’m going to build a decision tree model.” Others are like knobs or sliders, and you can use a little of that thing to a lot of it. In this latter category of choices, what’s important is that you’re looking for an amount of the thing that is just right. Technically speaking, the just-right amount creates a model that is neither <em>overfitted</em> or <em>underfitted</em>.</p>
<p>Let’s explore this idea with the <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> parameter to the <code class="docutils literal notranslate"><span class="pre">DecisionTreeRegressor</span></code> class, which is slider-like. This parameter limits the number of leaves the scikit-learn library can create in our decision tree, which you can think of as limiting the number of distinct house types in the decision tree. Your first thought might be, “Why would I want to limit the number of distinct house types? Wouldn’t more be better?” Well, let’s see!</p>
<p>The next code block runs a simple experiment: It builds seven decision-tree models, where each model is built using a different limit on the maximum number of leaf nodes that the library can create. Each model is trained and tested on the data set split from earlier. The code prints each model’s MAE, and when complete, it prints the value of <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> that produced the lowest MAE.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="c1">### chap17/ames.ipynb, 15th code block</span>
<span class="linenos"> 2</span><span class="c1"># Find the best model by varying the size of the decision tree</span>
<span class="linenos"> 3</span><span class="n">best_num_leaves</span> <span class="o">=</span> <span class="mi">0</span>
<span class="linenos"> 4</span><span class="n">lowest_mae</span> <span class="o">=</span> <span class="mf">9999999.9</span>
<span class="linenos"> 5</span>
<span class="linenos"> 6</span><span class="c1"># Run the experiment</span>
<span class="linenos"> 7</span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Leaves</span><span class="se">\t</span><span class="s1">MAE&#39;</span><span class="p">)</span>
<span class="linenos"> 8</span><span class="k">for</span> <span class="n">leaves</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">16384</span><span class="p">,</span> <span class="mi">131072</span><span class="p">]:</span>
<span class="linenos"> 9</span>    <span class="n">my_model</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_leaf_nodes</span><span class="o">=</span><span class="n">leaves</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="linenos">10</span>    <span class="n">my_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>
<span class="linenos">11</span>    <span class="n">predictions</span> <span class="o">=</span> <span class="n">my_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_X</span><span class="p">)</span>
<span class="linenos">12</span>    <span class="n">test_mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">test_y</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
<span class="linenos">13</span>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">leaves</span><span class="si">}</span><span class="se">\t</span><span class="s1">$</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">test_mae</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="linenos">14</span>
<span class="linenos">15</span>    <span class="k">if</span> <span class="n">test_mae</span> <span class="o">&lt;</span> <span class="n">lowest_mae</span><span class="p">:</span>
<span class="linenos">16</span>        <span class="c1"># Update best</span>
<span class="linenos">17</span>        <span class="n">best_num_leaves</span> <span class="o">=</span> <span class="n">leaves</span>
<span class="linenos">18</span>        <span class="n">lowest_mae</span> <span class="o">=</span> <span class="n">test_mae</span>
<span class="linenos">19</span>
<span class="linenos">20</span><span class="c1"># Report best</span>
<span class="linenos">21</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">BEST model uses </span><span class="si">{</span><span class="n">best_num_leaves</span><span class="si">}</span><span class="s1"> leaves&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition-you-try-it admonition">
<p class="admonition-title">You Try It</p>
<p>Before you run the code block above, guess which of the bounds (i.e., 4, 16, 64, 128, 1024, 16384, or 131072) will produce the model with the lowest MAE. Then run the code and check to see if you were right.</p>
</div>
<p>The experiment illustrates a common phenomenon:</p>
<ul class="simple">
<li><p>Too little of a thing, like leaves in a decision tree, produces a poor model. This probably makes sense to you because too few categories throws very different houses together in the same average. We saw this earlier when we tried to predict a price based on only a home’s number of bedrooms. Technically, the model underfits the training data; it doesn’t capture the patterns in the data set important to predicting the target variable.</p></li>
<li><p>Too much of a thing, perhaps paradoxically, also produces a poor model. The problem this time isn’t that the model didn’t capture the important patterns. It did. The problem is that it captured too many patterns, and in particular, the model is paying attention to patterns that exist only in the training data and not in the larger world. Technically, the model overfits the training data.</p></li>
</ul>
<p>Your best model will sit in the sweet spot between underfitting and overfitting. Starting with little of a thing, the MAE will decrease as you add more of that thing. At some point, however, as you continue to add more, the MAE will start to increase. With the split I created in my version of the Ames-Iowa-Housing data, the MAE was over 37,000 dollars with 4 leaves and decreased as I increased <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code>. However, it started increasing again at 128 leaves. My experiment’s best outcome for <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> was 64, producing a MAE that was 14 percent of the test set’s average home price. Your sister will be happier with this model than our first!</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>We lowered the MAE, but we haven’t necessarily found the best model. The last code block in <code class="docutils literal notranslate"><span class="pre">ames.ipynb</span></code> uses the <code class="docutils literal notranslate"><span class="pre">RandomForestRegressor</span></code> to create a model with an even lower MAE. You stop your search for lower MAEs when you have a model whose prediction accuracy is “good enough” for your problem domain.</p>
</div>
</section>
<section id="bias-in-ml">
<p><strong>Bias in ML.</strong> In terms of predictive accuracy, we’ve essentially solved your sister’s problem, but this metric isn’t the only one that matters. What do you think would happen if I called your sister and asked her to use the ML model you built to predict the selling price of my home in Boston? In other words, should a Boston homeowner trust the model’s output? And if not, why not?</p>
<p>Being a good real estate agent, your sister would know that she can’t help me. When I ask her my question, she’d almost certainly say, “My knowledge of home prices is specific to Ames, Iowa. Two identical houses, one in Ames and the other in Boston, will sell for different prices.<a class="footnote-reference brackets" href="#fn7" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a> I can’t help you.” And neither can your model, which like your sister, was trained exclusively on data specific to Ames, Iowa.</p>
<p>Data scientists would say that our model exhibits <em>representation bias</em>, which means that the data set we used to train it did not represent the area in which I live. This is only one kind of bias that can creep into our ML work, and <a class="reference external" href="https://arxiv.org/abs/1901.10002">Harini Suresh and John Guttag published an excellent paper in late 2021</a> that details six types of ML biases, including this one. Because ML is increasingly influencing important decisions in our lives, like the selling cost of our homes, we need to understand these different kinds of biases and evaluate our ML models for them.</p>
<p>Even though the model we built exhibits representation bias, it’s still a good model as long as we understand when we should and shouldn’t use it, as we just discussed. On the other hand, a model may contain biases that make it useless for its intended purpose, and if you’re solely focused on your model’s predictive accuracy, you may completely miss this and ship a model that does real harm to individuals and society. Let’s look at an instance of this with the help of Alexis Cook, who has written <a class="reference external" href="https://www.kaggle.com/code/alexisbcook/identifying-bias-in-ai/tutorial">a tutorial on kaggle titled “Identifying Bias in AI.”</a></p>
<p>The type of bias that she explores in the exercise at the end of her tutorial is called <em>historical bias</em>. The problem here is not that a training data set doesn’t represent the world in which we want to make predictions (i.e., representation bias), but that that world exhibits patterns we don’t want to use as the basis for future predictions.</p>
</section>
<section id="classifying-comments-as-toxic">
<p><strong>Classifying comments as toxic.</strong> Cook’s exercise uses part of a data set containing approximately 2 million public comments collected from a range of online news sites that used a civility plugin produced by <a class="reference external" href="https://medium.com/&#64;aja_15265/saying-goodbye-to-civil-comments-41859d3a2b1d">a now-defunct company called Civil Comments</a>. When the company shutdown, it released this data set, which was picked up by Alphabet’s Conversation AI team and turned into <a class="reference external" href="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview">a 2019 kaggle competition</a>. The goal of the competition was to build a ML model that could identify which comments in this data set were toxic and do so without discriminating against individuals because of their age, race, gender, religion, or other legally protected characteristics. This problem is hard because discrimination exists in our world, and it is reflected in many of the data sets we collect. We want a model that identifies toxic comments without also learning our society’s patterns of historical discrimination against particular individuals.</p>
<p>Cook steps you through the building of a <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> model, which takes a comment as a string and classifies it as toxic or not. You can check out the ML code in <a class="reference external" href="https://www.kaggle.com/code/alexisbcook/exercise-identifying-bias-in-ai/notebook">Cook’s tutorial</a>; my goal with you here is to focus on what makes identifying historical bias hard so that you aren’t caught unaware when you build your own ML models.</p>
<p>From her snapshot of the entire data set, Cook’s code pulls two example comments that illustrate toxic and non-toxic comments. I repeat below these two labeled comments:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span>Sample toxic comment: Too dumb to even answer.
<span class="linenos">2</span>Sample not-toxic comment: No they aren&#39;t.
</pre></div>
</div>
<p>Using supervised learning, Cook then fits a model to her training data, and she shows that this model achieves a 93-percent predictive accuracy on the test data. This is a high accuracy, and if we stopped evaluating the model after viewing this metric, we’d be in trouble.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Check for bias even if your model’s predictive accuracy is good. This chapter is about patterns, and don’t be lulled into complacency by thinking that a strong predictive accuracy is a sign of a good ML model. Representation bias often inversely correlates with predictive accuracy, but historical bias does not.</p>
</div>
<p>Cook’s tutorial then encourages you to classify these four comments using the model:</p>
<ol class="arabic simple">
<li><p>“I have a white friend”</p></li>
<li><p>“I have a black friend”</p></li>
<li><p>“I have a christian friend”</p></li>
<li><p>“I have a muslim friend”</p></li>
</ol>
<p>We know that none of these should be classified as toxic comments; yet the model classifies the first and third as non-toxic and the second and fourth as toxic.</p>
<p>What’s going on? Well, the world has historically discriminated against individuals because of their race and their religion. We know this, and if you look at the comments in the data set marked as toxic, they often involve discriminatory comments against Blacks and Muslims. The model classifies our second and fourth comments as toxic not because these words alone are toxic, but because they were often included in comments labeled as toxic. The model learned a discriminatory pattern and assumed it was important in classifying comments as toxic.</p>
<p>The result is a model that exacerbates the historical discrimination against a segment of our society. Despite its strong predictive accuracy on the test set, we should not deploy it.</p>
</section>
<section id="more-art-than-science">
<p><strong>More art than science.</strong> The toxicity-prediction model notwithstanding, data science has the capacity to do good and help people. This chapter is meant to get you started in this exciting field and make you aware of its biggest challenges.</p>
<p>To build a good model, experience and “taste” matter, as we saw in trying to choose which features to include in a model and where we’ll place a model’s sliders (e.g., maximum number of leaves in a decision tree). Because of this, the field remains more art than science, but everyday new libraries are released that make this work easier. Don’t get frustrated if your first models don’t perform well.</p>
<p>In this chapter, we built fairly good predictors using relatively simple statistical models. The benefit of simple models is that their predictions are generally easy to explain. For example, one house sells for more than another because it contains more bedrooms (everything else being equal), or a comment is considered toxic when it contains words often used in toxic comments. In a quest for ever more accurate predictors, data scientists are regularly developing more complicated models whose predictive performance we cannot as easily explain. Like predictive accuracy, a model’s explainability may be important in some application areas and not in others. For instance, I generally don’t think about how autocorrect comes up with its suggestions while I type my text messages, but predictive models won’t likely succeed in clinical medicine unless they are explainable. Do pay attention to what’s important in your application area.</p>
<p>Finally, a data scientist’s work is not complete until they attach a story to their models and associated discoveries. These stories require creativity, an adherence to the truth of what has been learned, and a thoughtfulness about how this information might be used for good and evil. May you use the knowledge you’ve gained to make the world a better place for all.</p>
<p>[Version 20240810]</p>
<hr class="footnotes docutils" />
<aside class="footnote brackets" id="fn1" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>You can read De Cock’s original paper in the <a class="reference external" href="https://jse.amstat.org/v19n3/decock.pdf">Journal of Statistics Education, Volume 19, Number 3 (2011)</a>. The copy of the Ames, Iowa Housing Data I use in this chapter is from <a class="reference external" href="https://www.kaggle.com/datasets/marcopale/housing">Marco Palermo’s Kaggle site</a>. Normally, as we discussed in Chapter 8, we’d want to inspect and clean a data set before we use it to build a prediction model, but De Cock has already cleaned the data.</p>
</aside>
<aside class="footnote brackets" id="fn2" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>Even the task of choosing the features (step 1) can be partially automated. If you take a ML course, you’ll learn about statistical techniques that can automatically prioritize the features in your data set.</p>
</aside>
<aside class="footnote brackets" id="fn3" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">3</a><span class="fn-bracket">]</span></span>
<p>“pandas — Python Data Analysis Library.” Accessed August 9, 2024. <a class="reference external" href="https://pandas.pydata.org/">https://pandas.pydata.org/</a></p>
</aside>
<aside class="footnote brackets" id="fn4" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">4</a><span class="fn-bracket">]</span></span>
<p>The Python module you want to import to use the scikit-learn library is called `sklearn`.</p>
</aside>
<aside class="footnote brackets" id="fn5" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">5</a><span class="fn-bracket">]</span></span>
<p>You can try this by calling `predict` with `train_X.head()` instead of `test_X` in the next code block.</p>
</aside>
<aside class="footnote brackets" id="fn6" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">6</a><span class="fn-bracket">]</span></span>
<p>This dollar amount reflects our knowledge that the average selling price in the Ames-Iowa-Housing data set is approximately 180,000 dollars.</p>
</aside>
<aside class="footnote brackets" id="fn7" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">7</a><span class="fn-bracket">]</span></span>
<p>For the major urban areas in the United States, the Council for Community and Economic Research (C2ER) collects and distributes cost-of-living data. At the end of 2023, the median home price was 335,000 dollars in Ames and 923,000 dollars in Boston, as computed by <a class="reference external" href="https://zerodown.com">zerodown.com</a>.</p>
</aside>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="chap16.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Chapter 16: Catch Them Early</p>
      </div>
    </a>
    <a class="right-next"
       href="chap18.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Chapter 18: Use Generative AI</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-home-prices">Predicting home prices</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#your-sister-s-data">Your sister’s data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-this-problem-ourselves">Solving this problem ourselves</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning">Machine learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#labeled-training-data">Labeled training data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ml-workflow">ML workflow</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-a-feel-for-the-data">Getting a feel for the data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#set-the-prediction-target">Set the prediction target</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pick-some-features">Pick some features</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fit-the-model-to-our-data">Fit the model to our data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-unseen-data">Predicting unseen data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-validation">Model validation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#making-the-fit-just-right">Making the fit just right</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-in-ml">Bias in ML</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classifying-comments-as-toxic">Classifying comments as toxic</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-art-than-science">More art than science</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Mike Smith
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023 by Michael D. Smith. All rights reserved.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>